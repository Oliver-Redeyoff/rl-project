{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd6bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "env = gym.make('Pong-v0', frameskip=4)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "state = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "#class for the neural network\n",
    "class neural_network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        #get number of actions & observations for output & input layer resp.\n",
    "        self.n_actions = 3\n",
    "        self.n_observations = 900\n",
    "        print(\"Number actions: \" + str(self.n_actions))\n",
    "        print(\"Number observations: \" + str(self.n_observations))\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=4, stride=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels = 4, kernel_size=4, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(324, 160),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(160, self.n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 324)  # reduce the dimensions for linear layer input\n",
    "        x = x.squeeze(0)\n",
    "        return self.classifier(x)\n",
    "                    \n",
    "    def predict(self, state):\n",
    "        #print(type(state))\n",
    "        action_probabilities = self.forward(state)\n",
    "        return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d620dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "def rgb2grey(rgb):\n",
    "    return np.dot(rgb[33:193][:][...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "from torchvision.transforms import functional\n",
    "\n",
    "def inv(frame):\n",
    "    new_frame = []\n",
    "    for line in frame:\n",
    "        new_frame.append([])\n",
    "        for item in line:\n",
    "            new_frame[-1].append(255-item)\n",
    "            \n",
    "    return torch.Tensor(new_frame)\n",
    "\n",
    "def process(rgb):\n",
    "    #greyscale, downscale, background removal\n",
    "    frame = rgb[34:194][:][:]\n",
    "    transposed = frame.transpose(2, 0, 1)\n",
    "    as_tensor = torch.Tensor(transposed)\n",
    "    grey = functional.rgb_to_grayscale(as_tensor)\n",
    "    downsampled = functional.resize(grey, [84, 84])\n",
    "    thresh = nn.Threshold(87.3, 0)\n",
    "    background_removed = thresh(downsampled)\n",
    "    state.append(background_removed)\n",
    "    \n",
    "    \n",
    "    if (len(state) > 2):\n",
    "        state.pop(0)\n",
    "    return torch.stack(state).squeeze(1)\n",
    "\n",
    "env.reset()\n",
    "for n in range(15):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "\n",
    "#taking a look at what the convolution actually looks like\n",
    "network = neural_network(env)\n",
    "for n in range(2):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "    imgplot = plt.imshow(img_pre[0], cmap=\"gray\")\n",
    "    plt.show()\n",
    "    img_pre = network.conv(process(observation)).detach().numpy()\n",
    "    print(np.shape(img_pre))\n",
    "    imgplot = plt.imshow(img_pre[0], cmap=\"gray\")\n",
    "    plt.show()\n",
    "    imgplot = plt.imshow(img_pre[1], cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for discounted return\n",
    "\n",
    "def calculate_discounted_return(rewards, discount_factor, secondary_discount_factor):\n",
    "    discounted_returns = [rewards[-1]]\n",
    "    #neg = False\n",
    "    \n",
    "    for i in range(len(rewards)-2, -1, -1):\n",
    "        if rewards[i] == 0:\n",
    "            discounted_returns.append(rewards[i] + (discount_factor*discounted_returns[-1]))\n",
    "        else:\n",
    "            discounted_returns.append(rewards[i] + (secondary_discount_factor*discounted_returns[-1]))\n",
    "    \n",
    "    discounted_returns.reverse()\n",
    "    return [discounted_return - (sum(discounted_returns) / len(discounted_returns)) for discounted_return in discounted_returns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cbef56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "random.seed(23)\n",
    "#env.seed(23) seems to be depricated \n",
    "torch.manual_seed(23)\n",
    "\n",
    "#initialise parameters\n",
    "alpha = 0.001\n",
    "discount_factor = 0.99\n",
    "secondary_discount_factor = 0\n",
    "n_network = neural_network(env)\n",
    "network_optimiser = optim.Adam(n_network.parameters(), lr=alpha)\n",
    "available_actions = [0,2,3]\n",
    "print(available_actions)\n",
    "\n",
    "num_episodes = 1000\n",
    "batch_size = 5\n",
    "episode_rewards = []\n",
    "total_rewards = [0]\n",
    "env_states = []\n",
    "total_loss = []\n",
    "batch_log_probs = []\n",
    "batch_actions = []\n",
    "batch_rewards = []\n",
    "batch_time = 0\n",
    "batch_count = 0\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(300,1000):\n",
    "    tic = time.time()\n",
    "    #initialising variables\n",
    "    episode_rewards.append(0)\n",
    "    observation = env.reset()\n",
    "    cur_state = process(observation)\n",
    "    cur_state = process(observation)\n",
    "    done = False\n",
    "    first = True\n",
    "    turn = 0\n",
    "\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        #generating episode trajectory\n",
    "        action_probs = n_network.predict(cur_state)\n",
    "        turn += 1\n",
    "        action = random.choices(available_actions, weights=action_probs.tolist())[0]\n",
    "        actions.append(action)\n",
    "        if action == 2:\n",
    "            action_index = 1\n",
    "        elif action == 3:\n",
    "            action_index = 2\n",
    "        else:\n",
    "            action_index = 0\n",
    "        #calculating log probability\n",
    "        log_probs.append(torch.log(action_probs.squeeze(0))[action_index])\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        cur_state = process(observation)\n",
    "        rewards.append(reward)\n",
    "        episode_rewards[episode] += reward\n",
    "\n",
    "        if done:\n",
    "            #adding to batches\n",
    "            discounted_returns = calculate_discounted_return(rewards, discount_factor, secondary_discount_factor)\n",
    "            batch_rewards.extend(discounted_returns)\n",
    "            batch_log_probs.extend(log_probs)\n",
    "            batch_count += 1\n",
    "            total_rewards[-1] += sum(rewards)\n",
    "            print(f\"Episode {episode}, {time.time() - tic:.2f} seconds, {episode_rewards[episode]} reward\")\n",
    "            batch_time += time.time() - tic\n",
    "\n",
    "            if batch_count == batch_size:\n",
    "                #processing batch\n",
    "                tic = time.time()\n",
    "                network_optimiser.zero_grad()\n",
    "\n",
    "                reward_tensor = torch.Tensor(batch_rewards)\n",
    "                chosen_log_probs = torch.stack(batch_log_probs)\n",
    "\n",
    "                log_x_reward = reward_tensor * chosen_log_probs\n",
    "                loss = - log_x_reward.mean()\n",
    "\n",
    "                loss.backward()\n",
    "                print(\"Loss: \", loss)\n",
    "                total_loss.append(loss)\n",
    "                network_optimiser.step()\n",
    "                \n",
    "                if episode % 100 == 99:\n",
    "                    print(\"Saving network\")\n",
    "                    torch.save(n_network.state_dict(), 'reinforce-logs/agent-final' + str(episode) + '.pt')\n",
    "                    with open(\"reinforce-logs/last-run.pickle\", \"wb\") as f:\n",
    "                        pickle.dump(total_rewards, f)\n",
    "                    with open(\"reinforce-logs/last-run-all.pickle\", \"wb\") as f:\n",
    "                        pickle.dump(episode_rewards, f)\n",
    "\n",
    "                batch_rewards = []\n",
    "                batch_log_probs = []\n",
    "                batch_count = 0\n",
    "                batch_time += time.time() - tic\n",
    "                print(f\"AVG REWARDS {episode}, {batch_time:.2f} SECONDS, {total_rewards[-1]/batch_size} AVG REWARD\")\n",
    "                batch_time = 0\n",
    "                total_rewards.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#transposed = observation.transpose(2, 0, 1\n",
    "\n",
    "conv2 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=8, stride=4)\n",
    "network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1))\n",
    "            #nn.ReLU())\n",
    "            #nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            #nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2),\n",
    "            #nn.ReLU())\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "            #nn.BatchNorm2d(16))\n",
    "\n",
    "networ2k = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=96, kernel_size=10, stride=3),  # (b x 96 x 55 x 55)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n",
    "            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n",
    "            #nn.LeakyReLU(),\n",
    "            #nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n",
    "            #nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "            #nn.LeakyReLU(),\n",
    "            #nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "            #nn.LeakyReLU(),\n",
    "            #nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n",
    "            #nn.LeakyReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n",
    "        )\n",
    "for n in range(10):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "\n",
    "last_space = [0,0]\n",
    "\n",
    "for n in range(10):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "grey_image = img_pre[0]\n",
    "print(\"GREY IMAGE PROCESS\")\n",
    "#print(np.amax(grey_image))\n",
    "imgplot = plt.imshow(grey_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(np.shape(grey_image))\n",
    "print(grey_image)\n",
    "\n",
    "fc = network(img_pre[None, ...].float())\n",
    "#print(max(fc))\n",
    "fc_numpy = fc.detach().numpy()\n",
    "print(np.shape(fc_numpy))\n",
    "\n",
    "#array = grey_image.detach().numpy()\n",
    "#print(np.shape(array))\n",
    "#result = numpy.where(array == numpy.amax(array))\n",
    "#print('Tuple of arrays returned : ', result)\n",
    "#print('Max value: ', numpy.amax(array))\n",
    "#print('List of coordinates of maximum value in Numpy array : ')\n",
    "# zip the 2 arrays to get the exact coordinates\n",
    "#listOfCordinates = list(zip(result[0], result[1]))\n",
    "# travese over the list of cordinates\n",
    "#for cord in listOfCordinates:\n",
    " #   print(cord)\n",
    "print(\"1\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy()), cmap=\"gray\")\n",
    "plt.show()\n",
    "'''\n",
    "print(\"2\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[1][:][:], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"3\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[2][:][:], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"4\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[3][:][:], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"5\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[4][:][:], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"6\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[5][:][:], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"7\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[6][:][:], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"8\")\n",
    "imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[7][:][:], cmap=\"gray\")\n",
    "plt.show()'''\n",
    "for n in range(50):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "    imgplot = plt.imshow(img_pre[0], cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "grey_image = img_pre[0]\n",
    "imgplot = plt.imshow(grey_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "#img_pre = process(observation)\n",
    "#fc = network(img_pre[None, ...].float())\n",
    "#print(np.shape(fc.detach().numpy()))\n",
    "#imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[1][:][:], cmap=\"gray\")\n",
    "#plt.show()\n",
    "\n",
    "for n in range(10):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "\n",
    "grey_image = img_pre[0]\n",
    "imgplot = plt.imshow(grey_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "#fc = network(img_pre[None, ...].float())\n",
    "#print(np.shape(fc.detach().numpy()))\n",
    "#imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[2][:][:], cmap=\"gray\")\n",
    "#plt.show()\n",
    "\n",
    "for n in range(10):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "    \n",
    "grey_image = img_pre[0]\n",
    "imgplot = plt.imshow(grey_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "#img_pre = process(observation)\n",
    "#fc = network(img_pre[None, ...].float())\n",
    "#print(np.shape(fc.detach().numpy()))\n",
    "#imgplot = plt.imshow(numpy.squeeze(fc.detach().numpy())[3][:][:], cmap=\"gray\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        #get number of actions & observations for output & input layer resp.\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_observations = 900\n",
    "        print(\"Number actions: \" + str(self.n_actions))\n",
    "        print(\"Number observations: \" + str(self.n_observations))\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_observations, 32), \n",
    "            #nn.ReLU(), \n",
    "            nn.Linear(32, self.n_actions),\n",
    "            nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=4, kernel_size=4, stride=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "           # nn.Conv2d(in_channels=4, out_channels = 8, kernel_size=5, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1600, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 400),\n",
    "            nn.Linear(400, self.n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #print(numpy.squeeze(x.detach().numpy())[1][:][:])\n",
    "        #imgplot = plt.imshow(numpy.squeeze(x.detach().numpy())[1][:][:], cmap=\"gray\")\n",
    "        #plt.show()\n",
    "        img_temp = numpy.squeeze(x.detach().numpy())[1][:][:]\n",
    "        unique, counts = np.unique(img_temp,return_counts=True)\n",
    "        common = unique[np.argmax(counts)]\n",
    "        self.Thresh = nn.Threshold(common, 0)\n",
    "        #print(\"max:\")\n",
    "        if common < 0:\n",
    "            common = math.floor(common)\n",
    "        else:\n",
    "            common = math.ceil(common)\n",
    "        #print(common)\n",
    "        x = self.Thresh(x)\n",
    "        #print(numpy.shape(x))\n",
    "        x = x.view(-1, 1600)  # reduce the dimensions for linear layer input\n",
    "        x = x.squeeze(0)\n",
    "        #print(numpy.shape(x))\n",
    "        return self.classifier(x)\n",
    "network_optimiser = optim.Adam(n_network.parameters(), lr=alpha)\n",
    "for n in range(20):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    img_pre = process(observation)\n",
    "\n",
    "network = neural_network(env)\n",
    "#print(img_pre)\n",
    "\n",
    "for n in range(10):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    img_pre = process(observation)\n",
    "    print(network(img_pre))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
