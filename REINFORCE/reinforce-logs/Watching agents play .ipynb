{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714b8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "#class for the neural network\n",
    "class neural_network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        #get number of actions & observations for output & input layer resp.\n",
    "        self.n_actions = 3\n",
    "        self.n_observations = 900\n",
    "        print(\"Number actions: \" + str(self.n_actions))\n",
    "        print(\"Number observations: \" + str(self.n_observations))\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=4, stride=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels = 4, kernel_size=4, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(324, 160),\n",
    "            nn.ReLU(),\n",
    "           # nn.Linear(800, 400),\n",
    "            nn.Linear(160, self.n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 324)  # reduce the dimensions for linear layer input\n",
    "        x = x.squeeze(0)\n",
    "        return self.classifier(x)\n",
    "                    \n",
    "    def predict(self, state):\n",
    "        #print(type(state))\n",
    "        action_probabilities = self.forward(state)\n",
    "        return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1f7f36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number actions: 3\n",
      "Number observations: 900\n",
      "ALPHA\n",
      "Episode 0, 5.43 seconds, 3344 frames, -13.0 reward\n",
      "Episode 1, 5.00 seconds, 3037 frames, -15.0 reward\n",
      "Episode 2, 4.13 seconds, 2500 frames, -20.0 reward\n",
      "Episode 3, 5.39 seconds, 3079 frames, -17.0 reward\n",
      "Episode 4, 5.06 seconds, 3136 frames, -12.0 reward\n",
      "Episode 5, 5.17 seconds, 3220 frames, -13.0 reward\n",
      "Episode 6, 4.59 seconds, 2818 frames, -16.0 reward\n",
      "Episode 7, 5.08 seconds, 3152 frames, -15.0 reward\n",
      "Episode 8, 4.93 seconds, 2975 frames, -18.0 reward\n",
      "Episode 9, 4.92 seconds, 2921 frames, -11.0 reward\n",
      "Episode 10, 6.20 seconds, 3799 frames, -12.0 reward\n",
      "Episode 11, 5.40 seconds, 3228 frames, -13.0 reward\n",
      "Episode 12, 4.42 seconds, 2717 frames, -17.0 reward\n",
      "Episode 13, 4.74 seconds, 2936 frames, -16.0 reward\n",
      "Episode 14, 5.21 seconds, 3244 frames, -14.0 reward\n",
      "Episode 15, 5.17 seconds, 3223 frames, -13.0 reward\n",
      "Episode 16, 5.92 seconds, 3618 frames, -12.0 reward\n",
      "Episode 17, 4.96 seconds, 3071 frames, -19.0 reward\n",
      "Episode 18, 4.73 seconds, 2928 frames, -16.0 reward\n",
      "Episode 19, 5.34 seconds, 3248 frames, -14.0 reward\n",
      "Episode 20, 4.57 seconds, 2843 frames, -15.0 reward\n",
      "Episode 21, 4.41 seconds, 2715 frames, -17.0 reward\n",
      "Episode 22, 4.83 seconds, 3011 frames, -14.0 reward\n",
      "Episode 23, 5.43 seconds, 3340 frames, -13.0 reward\n",
      "Episode 24, 5.04 seconds, 3155 frames, -15.0 reward\n",
      "Episode 25, 5.39 seconds, 3279 frames, -13.0 reward\n",
      "Episode 26, 4.86 seconds, 2975 frames, -13.0 reward\n",
      "Episode 27, 5.65 seconds, 3410 frames, -11.0 reward\n",
      "Episode 28, 5.68 seconds, 3377 frames, -12.0 reward\n",
      "Episode 29, 6.08 seconds, 3710 frames, -11.0 reward\n",
      "Episode 30, 5.86 seconds, 3537 frames, -16.0 reward\n",
      "Episode 31, 4.20 seconds, 2539 frames, -17.0 reward\n",
      "Episode 32, 5.48 seconds, 3092 frames, -15.0 reward\n",
      "Episode 33, 6.39 seconds, 2808 frames, -16.0 reward\n",
      "Episode 34, 7.76 seconds, 3242 frames, -14.0 reward\n",
      "Episode 35, 8.13 seconds, 3430 frames, -14.0 reward\n",
      "Episode 36, 6.66 seconds, 2905 frames, -15.0 reward\n",
      "Episode 37, 6.91 seconds, 3032 frames, -15.0 reward\n",
      "Episode 38, 7.63 seconds, 3362 frames, -14.0 reward\n",
      "Episode 39, 6.76 seconds, 2970 frames, -15.0 reward\n",
      "Episode 40, 7.56 seconds, 3047 frames, -16.0 reward\n",
      "Episode 41, 5.88 seconds, 2608 frames, -13.0 reward\n",
      "Episode 42, 7.80 seconds, 3369 frames, -14.0 reward\n",
      "Episode 43, 8.00 seconds, 3353 frames, -16.0 reward\n",
      "Episode 44, 7.47 seconds, 3142 frames, -12.0 reward\n",
      "Episode 45, 6.45 seconds, 2810 frames, -16.0 reward\n",
      "Episode 46, 7.42 seconds, 3153 frames, -15.0 reward\n",
      "Episode 47, 7.67 seconds, 3216 frames, -13.0 reward\n",
      "Episode 48, 6.40 seconds, 3203 frames, -15.0 reward\n",
      "Episode 49, 3.83 seconds, 2316 frames, -20.0 reward\n",
      "TEST 0 AVG REWARDS -14.62\n",
      "Time: 288.04\n",
      "Pre-Win Avg: 100.83699059561128\n",
      "Pre-Loss Avg: 115.76857142857143\n",
      "\n",
      "Number actions: 3\n",
      "Number observations: 900\n",
      "MODE 1\n",
      "Episode 0, 4.26 seconds, 2357 frames, -17.0 reward\n",
      "Episode 1, 4.78 seconds, 2840 frames, -17.0 reward\n",
      "Episode 2, 3.52 seconds, 2166 frames, -19.0 reward\n",
      "Episode 3, 4.35 seconds, 2295 frames, -17.0 reward\n",
      "Episode 4, 4.12 seconds, 2469 frames, -19.0 reward\n",
      "Episode 5, 4.44 seconds, 2752 frames, -16.0 reward\n",
      "Episode 6, 4.54 seconds, 2768 frames, -14.0 reward\n",
      "Episode 7, 3.89 seconds, 2466 frames, -19.0 reward\n",
      "Episode 8, 4.25 seconds, 2685 frames, -18.0 reward\n",
      "Episode 9, 4.06 seconds, 2505 frames, -18.0 reward\n",
      "Episode 10, 4.77 seconds, 2811 frames, -16.0 reward\n",
      "Episode 11, 4.05 seconds, 2452 frames, -16.0 reward\n",
      "Episode 12, 4.02 seconds, 2451 frames, -16.0 reward\n",
      "Episode 13, 4.95 seconds, 2938 frames, -14.0 reward\n",
      "Episode 14, 5.41 seconds, 3082 frames, -10.0 reward\n",
      "Episode 15, 4.26 seconds, 2503 frames, -18.0 reward\n",
      "Episode 16, 3.51 seconds, 2103 frames, -19.0 reward\n",
      "Episode 17, 3.95 seconds, 2387 frames, -16.0 reward\n",
      "Episode 18, 3.57 seconds, 2203 frames, -18.0 reward\n",
      "Episode 19, 4.07 seconds, 2476 frames, -17.0 reward\n",
      "Episode 20, 3.95 seconds, 2467 frames, -19.0 reward\n",
      "Episode 21, 4.20 seconds, 2654 frames, -17.0 reward\n",
      "Episode 22, 3.66 seconds, 2292 frames, -17.0 reward\n",
      "Episode 23, 3.18 seconds, 1981 frames, -19.0 reward\n",
      "Episode 24, 4.01 seconds, 2475 frames, -17.0 reward\n",
      "Episode 25, 4.47 seconds, 2802 frames, -18.0 reward\n",
      "Episode 26, 3.69 seconds, 2238 frames, -17.0 reward\n",
      "Episode 27, 3.26 seconds, 2076 frames, -18.0 reward\n",
      "Episode 28, 3.93 seconds, 2500 frames, -18.0 reward\n",
      "Episode 29, 4.00 seconds, 2513 frames, -16.0 reward\n",
      "Episode 30, 3.44 seconds, 2103 frames, -19.0 reward\n",
      "Episode 31, 4.22 seconds, 2619 frames, -18.0 reward\n",
      "Episode 32, 3.77 seconds, 2419 frames, -17.0 reward\n",
      "Episode 33, 3.39 seconds, 2040 frames, -19.0 reward\n",
      "Episode 34, 3.86 seconds, 2383 frames, -18.0 reward\n",
      "Episode 35, 4.19 seconds, 2561 frames, -18.0 reward\n",
      "Episode 36, 3.95 seconds, 2348 frames, -19.0 reward\n",
      "Episode 37, 3.97 seconds, 2486 frames, -15.0 reward\n",
      "Episode 38, 3.71 seconds, 2323 frames, -18.0 reward\n",
      "Episode 39, 3.75 seconds, 2388 frames, -16.0 reward\n",
      "Episode 40, 3.77 seconds, 2406 frames, -19.0 reward\n",
      "Episode 41, 4.52 seconds, 2790 frames, -20.0 reward\n",
      "Episode 42, 4.91 seconds, 2904 frames, -17.0 reward\n",
      "Episode 43, 3.96 seconds, 2468 frames, -19.0 reward\n",
      "Episode 44, 3.56 seconds, 2259 frames, -18.0 reward\n",
      "Episode 45, 4.65 seconds, 2914 frames, -15.0 reward\n",
      "Episode 46, 4.13 seconds, 2579 frames, -16.0 reward\n",
      "Episode 47, 4.67 seconds, 2849 frames, -15.0 reward\n",
      "Episode 48, 4.58 seconds, 2689 frames, -16.0 reward\n",
      "Episode 49, 3.26 seconds, 1929 frames, -19.0 reward\n",
      "TEST 1 AVG REWARDS -17.22\n",
      "Time: 203.39\n",
      "Pre-Win Avg: 79.84656084656085\n",
      "Pre-Loss Avg: 102.65142857142857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import functional\n",
    "import math\n",
    "import random\n",
    "import time \n",
    "\n",
    "available_actions = [0,2,3]\n",
    "\n",
    "state = []\n",
    "\n",
    "def process(rgb):\n",
    "    frame = rgb[34:194][:][:]\n",
    "    transposed = frame.transpose(2, 0, 1)\n",
    "    as_tensor = torch.Tensor(transposed)\n",
    "    grey = functional.rgb_to_grayscale(as_tensor)\n",
    "    downsampled = functional.resize(grey, [84, 84])\n",
    "    thresh = nn.Threshold(87.3, 0)\n",
    "    background_removed = thresh(downsampled)\n",
    "    state.append(background_removed)\n",
    "    \n",
    "    if (len(state) > 2):\n",
    "        state.pop(0)\n",
    "    #print(numpy.shape(torch.flatten(torch.stack(state).squeeze(1), 0, -1)))\n",
    "    return torch.stack(state).squeeze(1)\n",
    "\n",
    "env = gym.make('ALE/Pong-v5', frameskip=4)\n",
    "\n",
    "#saved_network = neural_network(env)\n",
    "#saved_network.load_state_dict(torch.load('agent-alpha.pt'))\n",
    "\n",
    "#observation = env.reset()\n",
    "#cur_state = process(observation)\n",
    "\n",
    "\n",
    "for test in range(3):\n",
    "    saved_network = neural_network(env)\n",
    "    if test == 0:\n",
    "        print(\"ALPHA\")\n",
    "        saved_network.load_state_dict(torch.load('agent-alpha.pt'))\n",
    "    elif test == 1:\n",
    "        print(\"MODE 1\")\n",
    "        saved_network.load_state_dict(torch.load('agent-mode1.pt'))\n",
    " #   for diff in range(4):\n",
    "    etic = time.time()\n",
    "    #env = gym.make('ALE/Pong-v5', frameskip=4, difficulty=diff, mode=test)\n",
    "    #env.seed()\n",
    "    observation = env.reset()\n",
    "    cur_state = process(observation)\n",
    "    pre_loss = []\n",
    "    pre_win = []\n",
    "\n",
    "    all_rewards = 0\n",
    "    for episode in range(50):\n",
    "        tic = time.time()\n",
    "        cur_state = process(env.reset())\n",
    "        done = False\n",
    "        rewards = [0]\n",
    "        memory = [cur_state]\n",
    "        total_reward = 0\n",
    "        frames = 0\n",
    "\n",
    "        while not done:\n",
    "            action_probs = saved_network.predict(cur_state)\n",
    "            action = random.choices(available_actions, weights=action_probs.tolist())[0]\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            frames += 1\n",
    "            total_reward += reward\n",
    "            if reward == 0:\n",
    "                rewards[-1] += 1\n",
    "            else:\n",
    "                if reward == 1:\n",
    "                    pre_win.append(rewards[-1])\n",
    "                elif reward == -1:\n",
    "                    pre_loss.append(rewards[-1])\n",
    "                #print(rewards[-1])\n",
    "                rewards.append(reward)\n",
    "                rewards.append(0)\n",
    "            cur_state = process(observation)\n",
    "            memory.append(cur_state[1])\n",
    "        all_rewards += total_reward\n",
    "        print(f\"Episode {episode}, {time.time() - tic:.2f} seconds, {frames} frames, {total_reward} reward\")\n",
    "           # print(rewards)\n",
    "            #print()\n",
    "    print(\"TEST \" + str(test) + \" AVG REWARDS \" + str(all_rewards/50))\n",
    "    print(f\"Time: {time.time() - etic:.2f}\")\n",
    "    print(\"Pre-Win Avg: \" + str(sum(pre_win) / len(pre_win)))\n",
    "    print(\"Pre-Loss Avg: \" + str(sum(pre_loss) / len(pre_loss)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49f9d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gym.wrappers.time_limit.TimeLimit'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08816257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lives': 0, 'episode_frame_number': 8730, 'frame_number': 94950}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ae068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
